{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=32, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.320206  [    0/60000]\n",
      "loss: 0.523348  [ 6400/60000]\n",
      "loss: 0.370266  [12800/60000]\n",
      "loss: 0.394129  [19200/60000]\n",
      "loss: 0.264072  [25600/60000]\n",
      "loss: 0.353106  [32000/60000]\n",
      "loss: 0.187043  [38400/60000]\n",
      "loss: 0.357616  [44800/60000]\n",
      "loss: 0.305204  [51200/60000]\n",
      "loss: 0.312075  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.212408 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.172859  [    0/60000]\n",
      "loss: 0.187457  [ 6400/60000]\n",
      "loss: 0.131854  [12800/60000]\n",
      "loss: 0.291529  [19200/60000]\n",
      "loss: 0.174217  [25600/60000]\n",
      "loss: 0.270751  [32000/60000]\n",
      "loss: 0.102506  [38400/60000]\n",
      "loss: 0.284239  [44800/60000]\n",
      "loss: 0.239804  [51200/60000]\n",
      "loss: 0.247922  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.155048 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.112985  [    0/60000]\n",
      "loss: 0.117993  [ 6400/60000]\n",
      "loss: 0.105089  [12800/60000]\n",
      "loss: 0.189570  [19200/60000]\n",
      "loss: 0.127416  [25600/60000]\n",
      "loss: 0.206800  [32000/60000]\n",
      "loss: 0.081080  [38400/60000]\n",
      "loss: 0.245446  [44800/60000]\n",
      "loss: 0.181902  [51200/60000]\n",
      "loss: 0.211699  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.126248 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.094299  [    0/60000]\n",
      "loss: 0.089384  [ 6400/60000]\n",
      "loss: 0.082252  [12800/60000]\n",
      "loss: 0.129234  [19200/60000]\n",
      "loss: 0.106756  [25600/60000]\n",
      "loss: 0.142308  [32000/60000]\n",
      "loss: 0.071003  [38400/60000]\n",
      "loss: 0.224343  [44800/60000]\n",
      "loss: 0.147266  [51200/60000]\n",
      "loss: 0.170876  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.111768 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.079870  [    0/60000]\n",
      "loss: 0.074129  [ 6400/60000]\n",
      "loss: 0.071489  [12800/60000]\n",
      "loss: 0.091239  [19200/60000]\n",
      "loss: 0.094095  [25600/60000]\n",
      "loss: 0.096100  [32000/60000]\n",
      "loss: 0.062321  [38400/60000]\n",
      "loss: 0.187272  [44800/60000]\n",
      "loss: 0.133573  [51200/60000]\n",
      "loss: 0.151641  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.102903 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.070365  [    0/60000]\n",
      "loss: 0.070698  [ 6400/60000]\n",
      "loss: 0.064195  [12800/60000]\n",
      "loss: 0.060909  [19200/60000]\n",
      "loss: 0.095982  [25600/60000]\n",
      "loss: 0.077702  [32000/60000]\n",
      "loss: 0.057603  [38400/60000]\n",
      "loss: 0.149050  [44800/60000]\n",
      "loss: 0.122957  [51200/60000]\n",
      "loss: 0.150527  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.097921 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.067523  [    0/60000]\n",
      "loss: 0.059365  [ 6400/60000]\n",
      "loss: 0.054667  [12800/60000]\n",
      "loss: 0.032005  [19200/60000]\n",
      "loss: 0.082251  [25600/60000]\n",
      "loss: 0.055493  [32000/60000]\n",
      "loss: 0.054958  [38400/60000]\n",
      "loss: 0.122539  [44800/60000]\n",
      "loss: 0.122305  [51200/60000]\n",
      "loss: 0.137842  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.097211 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.077855  [    0/60000]\n",
      "loss: 0.052093  [ 6400/60000]\n",
      "loss: 0.042284  [12800/60000]\n",
      "loss: 0.016231  [19200/60000]\n",
      "loss: 0.067610  [25600/60000]\n",
      "loss: 0.041505  [32000/60000]\n",
      "loss: 0.055669  [38400/60000]\n",
      "loss: 0.082949  [44800/60000]\n",
      "loss: 0.137778  [51200/60000]\n",
      "loss: 0.115855  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.098883 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.076249  [    0/60000]\n",
      "loss: 0.039130  [ 6400/60000]\n",
      "loss: 0.032718  [12800/60000]\n",
      "loss: 0.012178  [19200/60000]\n",
      "loss: 0.056010  [25600/60000]\n",
      "loss: 0.031125  [32000/60000]\n",
      "loss: 0.054704  [38400/60000]\n",
      "loss: 0.064778  [44800/60000]\n",
      "loss: 0.146714  [51200/60000]\n",
      "loss: 0.098495  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.104565 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.056336  [    0/60000]\n",
      "loss: 0.034159  [ 6400/60000]\n",
      "loss: 0.027695  [12800/60000]\n",
      "loss: 0.007934  [19200/60000]\n",
      "loss: 0.044412  [25600/60000]\n",
      "loss: 0.020876  [32000/60000]\n",
      "loss: 0.048578  [38400/60000]\n",
      "loss: 0.047411  [44800/60000]\n",
      "loss: 0.152264  [51200/60000]\n",
      "loss: 0.085592  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.110309 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kuanyu/Desktop/python/test/integertest.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kuanyu/Desktop/python/test/integertest.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload( \u001b[39m\"\u001b[39;49m\u001b[39mmodel.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:581\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    579\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 581\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    582\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    583\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    585\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.pth'"
     ]
    }
   ],
   "source": [
    "model = torch.load( \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsw0=model[\"linear_relu_stack.0.weight\"]\n",
    "lrsb0=model[\"linear_relu_stack.0.bias\"]\n",
    "lrsw2=model[\"linear_relu_stack.2.weight\"]\n",
    "lrsb2=model[\"linear_relu_stack.2.bias\"]\n",
    "lrsw4=model[\"linear_relu_stack.4.weight\"]\n",
    "lrsb4=model[\"linear_relu_stack.4.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50176])\n",
      "torch.Size([64])\n",
      "torch.Size([2048])\n",
      "torch.Size([32])\n",
      "torch.Size([320])\n"
     ]
    }
   ],
   "source": [
    "lrsw0=lrsw0.reshape(-1)\n",
    "lrsb0=lrsb0.reshape(-1)\n",
    "lrsw2=lrsw2.reshape(-1)\n",
    "lrsb2=lrsb2.reshape(-1)\n",
    "\n",
    "lrsw4=lrsw4.reshape(-1)\n",
    "\n",
    "\n",
    "print(lrsw0.shape)\n",
    "print(lrsb0.shape)\n",
    "print(lrsw2.shape)\n",
    "print(lrsb2.shape)\n",
    "\n",
    "print(lrsw4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_lrsw0=np.asarray(lrsw0.cpu())\n",
    "np_lrsb0=np.asarray(lrsb0.cpu())\n",
    "np_lrsw2=np.asarray(lrsw2.cpu())\n",
    "np_lrsb2=np.asarray(lrsb2.cpu())\n",
    "np_lrsw4=np.asarray(lrsw4.cpu())\n",
    "np_lrsb4=np.asarray(lrsb4.cpu())\n",
    "\n",
    "np.save(\"lrsw0\",np_lrsw0)\n",
    "np.save(\"lrsb0\",np_lrsb0)\n",
    "np.save(\"lrsw2\",np_lrsw2)\n",
    "np.save(\"lrsb2\",np_lrsb2)\n",
    "np.save(\"lrsw4\",np_lrsw4)\n",
    "np.save(\"lrsb4\",np_lrsb4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np=np.zeros((1000,784))\n",
    "lable_np=np.zeros(1000)\n",
    "for i in range(1000):\n",
    "    img,label=test_data[i]    \n",
    "    img_np[i]=np.asarray(img).reshape(-1)\n",
    "    lable_np[i]=label\n",
    "\n",
    "np.save(\"img\",img_np)\n",
    "np.save(\"label\",lable_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsw0=lrsw0.reshape(-1)\n",
    "lrsb0=lrsb0.reshape(-1)\n",
    "lrsw2=lrsw2.reshape(-1)\n",
    "lrsb2=lrsb2.reshape(-1)\n",
    "lrsw4=lrsw4.reshape(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50176])\n",
      "torch.Size([64])\n",
      "torch.Size([2048])\n",
      "torch.Size([32])\n",
      "torch.Size([320])\n"
     ]
    }
   ],
   "source": [
    "print(lrsw0.shape)\n",
    "print(lrsb0.shape)\n",
    "print(lrsw2.shape)\n",
    "print(lrsb2.shape)\n",
    "print(lrsw4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_lrsw0=np.asarray(lrsw0.cpu())\n",
    "np_lrsb0=np.asarray(lrsb0.cpu())\n",
    "np_lrsw2=np.asarray(lrsw2.cpu())\n",
    "np_lrsb2=np.asarray(lrsb2.cpu())\n",
    "np_lrsw4=np.asarray(lrsw4.cpu())\n",
    "np_lrsb4=np.asarray(lrsb4.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsw0_len=lrsw0.size(dim=0)\n",
    "lrsb0_len=lrsb0.size(dim=0)\n",
    "lrsw2_len=lrsw2.size(dim=0)\n",
    "lrsb2_len=lrsb2.size(dim=0)\n",
    "lrsw4_len=lrsw4.size(dim=0)\n",
    "lrsb4_len=lrsb4.size(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C++ program validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -4.92369461  -7.66485691  -0.98552698   1.12712586 -18.26786041\n",
      "  -3.78585649 -23.27987862  10.22897148  -3.02274179  -1.70486081]\n",
      "now: 0 hit: 1\n",
      "[-12.83986282   6.93760252  13.18755817   6.52630424 -25.29696465\n",
      "  -6.38394022 -15.25874615 -10.16630554  -2.74610853 -20.17750931]\n",
      "now: 1 hit: 2\n",
      "[-7.39804792  9.94595909 -1.10424376 -5.06233072 -2.85030746 -5.60874224\n",
      " -3.17454672  0.58857054 -1.30212307 -5.22882318]\n",
      "now: 2 hit: 3\n",
      "[ 11.11396503  -7.5778079   -1.61776733  -4.81179857 -15.71409988\n",
      "  -2.94385266  -4.53554869  -3.4479239  -12.57793713  -2.34860802]\n",
      "now: 3 hit: 4\n",
      "[-3.94347453 -9.67495155 -3.94474721 -6.39585495  9.06172466 -4.07384443\n",
      " -6.29758787 -1.76892364 -3.47377467  2.11102986]\n",
      "now: 4 hit: 5\n",
      "[-9.41569233 13.15397263 -3.75986767 -5.50849581 -3.9629488  -9.46448898\n",
      " -8.59140682  3.56472158 -3.70649743 -4.90757608]\n",
      "now: 5 hit: 6\n",
      "[-11.71393108 -12.06459713 -11.61721897  -5.48554516  11.11864376\n",
      "  -3.66878796 -10.74733162  -4.9389534    1.59216356  -2.54225016]\n",
      "now: 6 hit: 7\n",
      "[-12.44469643  -5.80890799  -6.57108545   2.11499119   0.15397474\n",
      "  -6.82124662 -20.6453228   -0.48285338  -4.07583714   7.19649649]\n",
      "now: 7 hit: 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kuanyu/Desktop/python/test/integertest.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kuanyu/Desktop/python/test/integertest.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(lrsb0_len):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kuanyu/Desktop/python/test/integertest.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m784\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kuanyu/Desktop/python/test/integertest.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         out1[i]\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mlrsw0[i\u001b[39m*\u001b[39m\u001b[39m784\u001b[39m\u001b[39m+\u001b[39mj]\u001b[39m*\u001b[39mimg[j]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kuanyu/Desktop/python/test/integertest.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     out1[i]\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mlrsb0[i]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kuanyu/Desktop/python/test/integertest.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(lrsb0_len):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hit=0\n",
    "for r in range(100):\n",
    "    img,label=test_data[r]\n",
    "    img=img.reshape(-1)\n",
    "    #img=np.ones(784)*0.5\n",
    "    out1=np.zeros(lrsb0_len)\n",
    "    out2=np.zeros(lrsb2_len)\n",
    "    out3=np.zeros(lrsb4_len)\n",
    "    \n",
    "    for i in range(lrsb0_len):\n",
    "        for j in range(784):\n",
    "            out1[i]+=lrsw0[i*784+j]*img[j]\n",
    "        out1[i]+=lrsb0[i]\n",
    "    for i in range(lrsb0_len):\n",
    "        if(out1[i]<0):\n",
    "            out1[i]=0\n",
    "\n",
    "    for i in range(lrsb2_len):\n",
    "        for j in range(lrsb0_len):\n",
    "            out2[i]+=lrsw2[i*lrsb0_len+j]*out1[j]\n",
    "        out2[i]+=lrsb2[i]\n",
    "    for i in range(lrsb2_len):\n",
    "        if(out2[i]<0):\n",
    "            out2[i]=0\n",
    "\n",
    "    for i in range(lrsb4_len):\n",
    "        for j in range(lrsb2_len):\n",
    "            out3[i]+=lrsw4[i*lrsb2_len+j]*out2[j]\n",
    "        out3[i]+=lrsb4[i]\n",
    "    if(out3.argmax()==label):\n",
    "        hit+=1\n",
    "    print(out3)\n",
    "    #print(out1)\n",
    "    #print(out2)\n",
    "    print(\"now:\",r,\"hit:\",hit)\n",
    "print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx ,ele in enumerate(lrsw0):\n",
    "    lrsw0[idx]=int((ele+1)/2*255-127)\n",
    "for idx ,ele in enumerate(lrsb0):\n",
    "    lrsb0[idx]=int((ele+1)/2*255-127)\n",
    "for idx ,ele in enumerate(lrsw2):\n",
    "    lrsw2[idx]=int((ele+1)/2*255-127)\n",
    "for idx ,ele in enumerate(lrsb2):\n",
    "    lrsb2[idx]=int((ele+1)/2*255-127)\n",
    "for idx ,ele in enumerate(lrsw4):\n",
    "    lrsw4[idx]=int((ele+1)/2*255-127)\n",
    "for idx ,ele in enumerate(lrsb4):\n",
    "    lrsb4[idx]=int((ele+1)/2*255-127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn img from float to int\n",
    "def int_conver(np_a):\n",
    "    for idx,ele in enumerate(np_a):\n",
    "        np_a[idx]=int((ele+1)/2*255-127)\n",
    "\n",
    "    return np_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test overflow underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -70624.  -98242.  -10629.   34992. -274669.  -57607. -335702.  152348.\n",
      "  -34606.  -30507.]\n",
      "now: 0 hit: 1\n",
      "[-217811.  122613.  199276.  119135. -416930.  -86434. -239267. -172297.\n",
      "  -33127. -306899.]\n",
      "now: 1 hit: 2\n",
      "[-88264. 137708.  -7069. -66690. -41208. -64176. -30171.   7647. -18517.\n",
      " -78782.]\n",
      "now: 2 hit: 3\n",
      "[ 194821. -126154.  -23870.  -58429. -236840.  -62416.  -80430.  -59965.\n",
      " -205584.  -45231.]\n",
      "now: 3 hit: 4\n",
      "[ -64593. -129611.  -56257.  -69673.  135720.  -57840.  -91932.  -27020.\n",
      "  -45389.   23976.]\n",
      "now: 4 hit: 5\n",
      "[-117884.  191689.  -48317.  -68117.  -42777. -131649. -100324.   47537.\n",
      "  -57112.  -69832.]\n",
      "now: 5 hit: 6\n",
      "[-168924. -166428. -176691.  -69623.  148713.  -26253. -134392.  -76397.\n",
      "   30688.  -53954.]\n",
      "now: 6 hit: 7\n",
      "[-187435.  -91829.  -95538.   54594.  -14310.  -92280. -321222.   -8418.\n",
      "  -64570.  106749.]\n",
      "now: 7 hit: 8\n",
      "[ -91873.  -72935. -100869.  -86448.  -49622.   83270.   40498. -152648.\n",
      "  -11853.  -22223.]\n",
      "now: 8 hit: 9\n",
      "[-192363. -198178. -235024.   -1136.   44732. -160234. -454706.   40572.\n",
      "   17331.  222370.]\n",
      "now: 9 hit: 10\n",
      "[ 199709. -176713.    1195. -118952. -186373.  -44012.  -31018. -122370.\n",
      " -122026.  -45519.]\n",
      "now: 10 hit: 11\n",
      "[ -38869. -122748.  -65432.  -95425.  -15304.   18159.  131259. -133927.\n",
      "  -26574. -197038.]\n",
      "now: 11 hit: 12\n",
      "[-133931. -151407. -169762.    3417.    9259.  -87231. -310656.  -20552.\n",
      "    2089.  177384.]\n",
      "now: 12 hit: 13\n",
      "[ 185495. -200936.  -10617. -134435. -161119.  -19746.  -89630.    8049.\n",
      " -103210.  -47032.]\n",
      "now: 13 hit: 14\n",
      "[-164426.  196288. -143420.  -11618.  -74877. -125642. -119941.  -41320.\n",
      "  -20629.  -81573.]\n",
      "now: 14 hit: 15\n",
      "[-195277.  -29478. -134456.   97563. -186032.  166001. -176158. -189605.\n",
      "  -29590.  -72719.]\n",
      "now: 15 hit: 16\n",
      "[ -67114. -170689.  -77949.  -64394.   32579. -113998. -249790.    4461.\n",
      "  -27485.  144317.]\n",
      "now: 16 hit: 17\n",
      "[  -3187. -107761.   58000.   -1510. -357421.  -62095. -319184.  190420.\n",
      " -160209.  -77932.]\n",
      "now: 17 hit: 18\n",
      "[-198512.  -74225.  -58155.   83609.  -83615.   13433. -170652. -197117.\n",
      "   34938.  -57162.]\n",
      "now: 18 hit: 19\n",
      "[ -88909. -142110. -133877.  -46164.  145984.  -51191.  -89212.  -23328.\n",
      "  -51060.  -29996.]\n",
      "now: 19 hit: 20\n",
      "[-132408. -148365. -244032.   13725.  -49467.  -35328. -415896.    5279.\n",
      "  -87202.  121217.]\n",
      "now: 20 hit: 21\n",
      "[ -93492.  -73412. -176221.  -74535.  -73147.  112523.  102900. -256504.\n",
      "   34532.  -83950.]\n",
      "now: 21 hit: 21\n",
      "[  20118. -104186.  -57615. -121095.  -20749.   24164.  181809.  -72910.\n",
      "  -57008. -191613.]\n",
      "now: 22 hit: 22\n",
      "[-216073.  -60697. -236157.   13214. -190621.  244013. -109075. -270779.\n",
      "  -34417.     600.]\n",
      "now: 23 hit: 23\n",
      "[ -80105. -113759.  -85319.  -34170.   93626.  -11291. -127650.   22563.\n",
      "  -58409.   13146.]\n",
      "now: 24 hit: 24\n",
      "[ 2.58512e+05 -3.07749e+05 -4.85850e+04 -2.53044e+05 -2.69000e+02\n",
      " -1.08547e+05  2.54470e+04 -1.26362e+05 -1.58777e+05 -7.38430e+04]\n",
      "now: 25 hit: 25\n",
      "[ -27359.  -47809.   14318.  -26916. -206697.    6282. -203857.  128592.\n",
      " -102254.    5346.]\n",
      "now: 26 hit: 26\n",
      "[-155857. -167024. -123983.  -46924.  194095.  -66716. -174601.  -32780.\n",
      "  -54244.   33115.]\n",
      "now: 27 hit: 27\n",
      "[ 192651. -214358.   29135.  -58576. -384698.  -27659. -190208. -106328.\n",
      "  -86077.  -74303.]\n",
      "now: 28 hit: 28\n",
      "[-194633.  143916.  -71702.  -12887.  -26908.  -88089.  -97275.  -36824.\n",
      "  -28457. -115814.]\n",
      "now: 29 hit: 29\n",
      "[-365923.  -50961. -253005.  232432. -302694.   44314. -505881. -128410.\n",
      "  -36376.    9177.]\n",
      "now: 30 hit: 30\n",
      "[-147597.   80765.  -68348.    1933.  -11100.  -55899.  -76113.  -49839.\n",
      "    -621.  -17170.]\n",
      "now: 31 hit: 31\n",
      "[-344589.  -46823. -283422.  218556. -214341.   90657. -341050. -226216.\n",
      "  -43163.  -45782.]\n",
      "now: 32 hit: 32\n",
      "[  12255. -197626.  -85565. -103937.   80688.    1931.  -42364.  -14350.\n",
      "  -88798.  -83632.]\n",
      "now: 33 hit: 33\n",
      "[-164334.   52917.  112000.   69824. -292648. -241771. -388964.  231595.\n",
      "  -40351. -132095.]\n",
      "now: 34 hit: 34\n",
      "[-204107.  104249.  250746.   10166. -290853. -109891. -183228.  -93957.\n",
      "   -1451. -205576.]\n",
      "now: 35 hit: 35\n",
      "[ -66021.   -8299.  106127.  -22134. -292436. -106741. -238076.  163858.\n",
      "  -77397.  -83038.]\n",
      "now: 36 hit: 36\n",
      "[-164469.  164087. -103597.  -67204.  -44049.  -92960.  -67629.  -19826.\n",
      "   -2442.  -54529.]\n",
      "now: 37 hit: 37\n",
      "[ -79213.   14099.   49807.   39171. -296962.  -11437.  -81986. -209494.\n",
      "   -6619. -181856.]\n",
      "now: 38 hit: 38\n",
      "[-124415.  201789. -164321.  -44100.  -40102. -162196. -111934.  -62580.\n",
      "  -14097.  -77499.]\n",
      "now: 39 hit: 39\n",
      "[-78983.  92538. -33919. -28928. -45551. -50839. -42055.  -3941.  -1015.\n",
      " -46475.]\n",
      "now: 40 hit: 40\n",
      "[ -77680.   20682.   35376.   36206. -206100.  -90736. -193949.  120652.\n",
      "  -37825.  -57791.]\n",
      "now: 41 hit: 41\n",
      "[-187663. -206086. -251500.  -58800.  203086.  -93210. -253969.  -22392.\n",
      "  -51622.   45580.]\n",
      "now: 42 hit: 42\n",
      "[-126056.   28467.  102230.   22082.  -23596.  -63314.   -7005.  -23996.\n",
      "  -47186. -231977.]\n",
      "now: 43 hit: 43\n",
      "[-263960.   -8984.  -79205.  142109. -154910.    2182. -239800. -138781.\n",
      "  -49861. -138461.]\n",
      "now: 44 hit: 44\n",
      "[-149184. -100867. -247585.   32411. -236331.  231392.  -93536. -281443.\n",
      "   -7115.  -34725.]\n",
      "now: 45 hit: 45\n",
      "[-256143.  112780.  -90238.   25056.  -80937.  -39113. -115917. -171871.\n",
      "    3695.  -18635.]\n",
      "now: 46 hit: 46\n",
      "[-132320.   19743.  185956.   -4243.  -25665. -153017.  -16870.   -6526.\n",
      "  -80569. -246184.]\n",
      "now: 47 hit: 47\n",
      "[-295640. -278835. -360319.  -45988.  252323.   -4078. -280812. -124115.\n",
      "  -25703.   42561.]\n",
      "now: 48 hit: 48\n",
      "[ -94580. -147992. -100983.  -92380.  169500. -108067. -111642.  -31167.\n",
      "  -46332.   14427.]\n",
      "now: 49 hit: 49\n",
      "[   6090.  -80347. -113252.  -43169.  -92258.   87384.  124827. -210616.\n",
      "  -54113. -158902.]\n",
      "now: 50 hit: 50\n",
      "[-334359.  -56716. -115528.  224076. -159495.  -25111. -368737. -170380.\n",
      "  -43246. -232069.]\n",
      "now: 51 hit: 51\n",
      "[-162096. -115197. -265500.  -27473. -121392.  203494. -120409. -168486.\n",
      "  -25961.   11043.]\n",
      "now: 52 hit: 52\n",
      "[-1.85100e+05 -5.50070e+04 -1.71179e+05  5.92580e+04 -1.41173e+05\n",
      "  1.59748e+05 -1.39598e+05 -1.92498e+05 -1.91000e+03 -1.52000e+02]\n",
      "now: 53 hit: 53\n",
      "[ -46587. -155326.  -42163. -171214.  -44028.   50170.  186621. -232736.\n",
      "   16015. -196055.]\n",
      "now: 54 hit: 54\n",
      "[ 166823. -178478.  -40332. -135571. -168349.    4093.   22748. -175207.\n",
      "   12316.  -48966.]\n",
      "now: 55 hit: 55\n",
      "[-199435. -263116. -217483.  -94014.  260248.  -55455. -212273.  -27541.\n",
      "  -61091.   -5575.]\n",
      "now: 56 hit: 56\n",
      "[ -62909.  156933.  -17530.  -46836.  -42447. -108899.  -52349.   12487.\n",
      "  -45291.  -68851.]\n",
      "now: 57 hit: 57\n",
      "[-141872. -190567. -116043.  -27830.   31630.  -82502. -343310.  -16969.\n",
      "  -20189.  210575.]\n",
      "now: 58 hit: 58\n",
      "[-55706. -40864. -57708. -68959. -63951.  81803. -79429. -16035.  10622.\n",
      " -46578.]\n",
      "now: 59 hit: 59\n",
      "[ -26092.  -36456.   26121.    5444. -325626. -108584. -317559.  224280.\n",
      " -182268.  -27976.]\n",
      "now: 60 hit: 60\n",
      "[ -86260.  -30862.  113966.  -47685. -145235.  -60028.  -85474. -129725.\n",
      "   89405. -120921.]\n",
      "now: 61 hit: 60\n",
      "[-112181.  -98273. -130429.  -34299.  -30842.   70240. -126192.  -86633.\n",
      "   13492.   65955.]\n",
      "now: 62 hit: 60\n",
      "[-162621.    7488.  -27712.  148506. -201462. -174433. -356659.  -87761.\n",
      "  -54318.  -52739.]\n",
      "now: 63 hit: 61\n",
      "[-232284. -162204.  -61113.  102056. -171955. -236543. -640994.  234495.\n",
      " -150367.     950.]\n",
      "now: 64 hit: 62\n",
      "[-116127.  -61308. -135573.    5073.   49852.  -17303. -100963. -111581.\n",
      "  -24426.   41961.]\n",
      "now: 65 hit: 63\n",
      "[ -25176.  -28673.   36153.  -86809.  -19963.  -81098.   70382.   31919.\n",
      "  -94938. -146454.]\n",
      "now: 66 hit: 64\n",
      "[ -94170. -134489.  -49485.  -68320.  158727.  -91209. -118754.  -44251.\n",
      "  -44814.    7944.]\n",
      "now: 67 hit: 65\n",
      "[-363420.  -79285. -247470.  272107. -270499.  -33087. -496859. -206598.\n",
      "  -22859.  -70930.]\n",
      "now: 68 hit: 66\n",
      "[ 194613. -180677.  -70240. -146274. -154240.   36888.  -18089.   18359.\n",
      " -185706. -108017.]\n",
      "now: 69 hit: 67\n",
      "[ -22850.  -95585.   21096.   15148. -349092.  -93083. -358424.  227596.\n",
      " -145098.  -48400.]\n",
      "now: 70 hit: 68\n",
      "[ 316876. -247647.   -2277. -218893. -180540.  -86892.   34758. -121644.\n",
      " -162356.  -71125.]\n",
      "now: 71 hit: 69\n",
      "[ -97561.   -6524.  175953.   84691. -315043.  -54274. -220572. -139341.\n",
      "   -8973. -175924.]\n",
      "now: 72 hit: 70\n",
      "[ -94197.  -81807.  -86145.  -13191.  -95197. -262561. -395990.   41463.\n",
      "  -24932.  136582.]\n",
      "now: 73 hit: 71\n",
      "[-163621.  146053. -137598.  -92241.  -34295.  -75884.  -72600.  -26219.\n",
      "  -15050.  -49249.]\n",
      "now: 74 hit: 72\n",
      "[-116142.   17961.   36344.   41241. -148515. -156676. -245073.  170144.\n",
      "  -59086.  -48214.]\n",
      "now: 75 hit: 73\n",
      "[-240706.    4420. -164261.  148227. -140743.   48554. -172045. -193107.\n",
      "    4375. -117232.]\n",
      "now: 76 hit: 74\n",
      "[ -79472.    8525.   95265.  -22253. -181261.  -31628.  -65589.  -58112.\n",
      "    9891.  -69764.]\n",
      "now: 77 hit: 75\n",
      "[ -99005.  -28980. -194639.  -43984.   21785. -135366. -236582.  -58133.\n",
      "  -18314.   88240.]\n",
      "now: 78 hit: 76\n",
      "[-135857.  -90284.   -4982.    4142. -287847. -198647. -480881.  213614.\n",
      "  -28279.   10629.]\n",
      "now: 79 hit: 77\n",
      "[ -88973.  -90264. -166688.  -25223.  -52889.   56088. -207020.   48062.\n",
      " -102105.   49807.]\n",
      "now: 80 hit: 77\n",
      "[ -52037.  -43995. -102918. -126234.  -84645.   69124.  150137. -216639.\n",
      "   -3774.  -81290.]\n",
      "now: 81 hit: 78\n",
      "[-181452.   24747.  287245.   33827. -185158. -201920. -197897.  -54435.\n",
      "  -56012. -241048.]\n",
      "now: 82 hit: 79\n",
      "[ -60872.  -45697.  -29106.  -38519. -130605.  -14720. -222439.  131487.\n",
      " -112966.   44388.]\n",
      "now: 83 hit: 80\n",
      "[ -99123.  -70708. -113580.  -24139.  -51053.   22064.  -58223. -102945.\n",
      "  107013. -102989.]\n",
      "now: 84 hit: 81\n",
      "[-265372. -283987. -297694.  -62624.  279588.  -84981. -284255.  -41531.\n",
      "  -43494.   -6047.]\n",
      "now: 85 hit: 82\n",
      "[-104019.  -39073.   18046.    8643. -275225. -147900. -356845.  188179.\n",
      "  -68152.  -23449.]\n",
      "now: 86 hit: 83\n",
      "[-236999.  -91377. -176497.  182636. -120931.   58163. -301632.  -81308.\n",
      " -187016. -134932.]\n",
      "now: 87 hit: 84\n",
      "[ -39639. -165222.  -73885. -193586.   33653.   20228.  230302. -132476.\n",
      "  -75566. -274074.]\n",
      "now: 88 hit: 85\n",
      "[-1.57918e+05  1.38401e+05 -7.20460e+04 -6.24840e+04 -3.60220e+04\n",
      " -9.01840e+04 -7.06650e+04  1.05780e+04 -1.47000e+02 -1.66878e+05]\n",
      "now: 89 hit: 86\n",
      "[-159125.  -46659. -111002.  189145. -320121.   22758. -299503. -214679.\n",
      "   20341.  -23742.]\n",
      "now: 90 hit: 87\n",
      "[ -84054.  -62153.  -90984. -109333.  -90233.   91206.  179542. -222896.\n",
      "   16644. -170789.]\n",
      "now: 91 hit: 88\n",
      "[ -94868.  -26291.  -80667.  -88081.   73081.  -95455. -170810.   -9923.\n",
      "  -36621.   89431.]\n",
      "now: 92 hit: 89\n",
      "[-288528. -100640. -257866.  215214. -342780.   63596. -503065. -224295.\n",
      "  -53511.    3007.]\n",
      "now: 93 hit: 90\n",
      "[-197216.  198795.  -87589. -109620.   -4668. -162203.  -55916.  -74560.\n",
      "    -676.  -70257.]\n",
      "now: 94 hit: 91\n",
      "[-112655. -201504. -158407. -101588.   83347.   -2204. -113856. -139795.\n",
      "   17875.   74305.]\n",
      "now: 95 hit: 92\n",
      "[-151707.   49989. -101226.     591.   27331.  -65505.  -89739.  -56696.\n",
      "   12019.    -894.]\n",
      "now: 96 hit: 93\n",
      "[-161120.  -35436.   28201.   47609.  -89644. -232838. -240387.  162669.\n",
      "  -55048. -185324.]\n",
      "now: 97 hit: 94\n",
      "[  22386.  -58309. -134237. -128154.  -69957.   68594.  154447. -182700.\n",
      "  -37797.  -62197.]\n",
      "now: 98 hit: 95\n",
      "[-125946. -237382. -119752.  -25540.  -60920.  -97937. -430790.  -19511.\n",
      "  -64751.  224346.]\n",
      "now: 99 hit: 96\n",
      "96\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hit=0\n",
    "for r in range(100):\n",
    "    img,label=test_data[r]\n",
    "    img=img.reshape(-1)\n",
    "    img=int_conver(img)\n",
    "    #img=np.ones(784)\n",
    "    out1=np.zeros(lrsb0_len)\n",
    "    out2=np.zeros(lrsb2_len)\n",
    "    out3=np.zeros(lrsb4_len)\n",
    "    \n",
    "    for i in range(lrsb0_len):\n",
    "        for j in range(784):\n",
    "            out1[i]+=lrsw0[i*784+j]*img[j]\n",
    "        out1[i]+=lrsb0[i]\n",
    "    for i in range(lrsb0_len):\n",
    "        if(out1[i]<0):\n",
    "            out1[i]=0\n",
    "##########debug begin #################\n",
    "    for ele in out1:\n",
    "        if(ele>0 and ele>2147483647):\n",
    "            print(\"overflow out1:\",out1)\n",
    "    for ele in out1:\n",
    "        if(ele<0 and ele<-2147483648):\n",
    "            print(\"underflow out1:\",out1) \n",
    "    for idx,ele in enumerate(out1):\n",
    "        out1[idx]=int((out1[idx]/2**14))\n",
    "    \n",
    "##########debug end#################\n",
    "    for i in range(lrsb2_len):\n",
    "        for j in range(lrsb0_len):\n",
    "            out2[i]+=lrsw2[i*lrsb0_len+j]*out1[j]\n",
    "        out2[i]+=lrsb2[i]\n",
    "    for i in range(lrsb2_len):\n",
    "        if(out2[i]<0):\n",
    "            out2[i]=0\n",
    "##########debug begin #################\n",
    "\n",
    "    for ele in out2:\n",
    "        if(ele>0 and ele>2147483647):\n",
    "            print(\"overflow out2:\",out2)\n",
    "    for ele in out2:\n",
    "        if(ele<0 and ele<-2147483648):\n",
    "            print(\"underflow out2:\",out2) \n",
    "\n",
    "\n",
    "\n",
    "##########debug end#################\n",
    "    for i in range(lrsb4_len):\n",
    "        for j in range(lrsb2_len):\n",
    "            out3[i]+=lrsw4[i*lrsb2_len+j]*out2[j]\n",
    "        out3[i]+=lrsb4[i]\n",
    "##########debug begin #################\n",
    "    for ele in out3:\n",
    "        if(ele>0 and ele>2147483647):\n",
    "            print(\"overflow out3:\",out3)\n",
    "    for ele in out3:\n",
    "        if(ele<0 and ele<-2147483648):\n",
    "            print(\"underflow out3:\",out3) \n",
    "\n",
    "##########debug end##########\n",
    "    if(out3.argmax()==label):\n",
    "        hit+=1\n",
    "    print(out3)\n",
    "    #print(out1)\n",
    "    #print(out2)\n",
    "    print(\"now:\",r,\"hit:\",hit)\n",
    "print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate cpp file for c sim in vitis hls & cpp file for ROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_lrsw0=np.asarray(lrsw0.cpu())\n",
    "np_lrsb0=np.asarray(lrsb0.cpu())\n",
    "np_lrsw2=np.asarray(lrsw2.cpu())\n",
    "np_lrsb2=np.asarray(lrsb2.cpu())\n",
    "np_lrsw4=np.asarray(lrsw4.cpu())\n",
    "np_lrsb4=np.asarray(lrsb4.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('lrsw0.cpp','w')\n",
    "f.write(\"int lrsw0[]={\")\n",
    "for i in range(np_lrsw0.shape[0]):\n",
    "    f.write(str(int(np_lrsw0[i])))\n",
    "    if(i==np_lrsw0.shape[0]-1):\n",
    "        pass\n",
    "    else:\n",
    "        f.write(',')\n",
    "f.write('};')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('lrsb0.cpp','w')\n",
    "f.write(\"int lrsb0[]={\")\n",
    "for i in range(np_lrsb0.shape[0]):\n",
    "    f.write(str(int(np_lrsb0[i])))\n",
    "    if(i==np_lrsb0.shape[0]-1):\n",
    "        pass\n",
    "    else:\n",
    "        f.write(',')\n",
    "f.write('};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('lrsw2.cpp','w')\n",
    "f.write(\"int lrsw2[]={\")\n",
    "for i in range(np_lrsw2.shape[0]):\n",
    "    f.write(str(int(np_lrsw2[i])))\n",
    "    if(i==np_lrsw2.shape[0]-1):\n",
    "        pass\n",
    "    else:\n",
    "        f.write(',')\n",
    "f.write('};')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('lrsb2.cpp','w')\n",
    "f.write(\"int lrsb2[]={\")\n",
    "for i in range(np_lrsb2.shape[0]):\n",
    "    f.write(str(int(np_lrsb2[i])))\n",
    "    if(i==np_lrsb2.shape[0]-1):\n",
    "        pass\n",
    "    else:\n",
    "        f.write(',')\n",
    "f.write('};')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('lrsb4.cpp','w')\n",
    "f.write(\"int lrsb4[]={\")\n",
    "for i in range(np_lrsb4.shape[0]):\n",
    "    f.write(str(int(np_lrsb4[i])))\n",
    "    if(i==np_lrsb4.shape[0]-1):\n",
    "        pass\n",
    "    else:\n",
    "        f.write(',')\n",
    "f.write('};')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('lrsw4.cpp','w')\n",
    "f.write(\"int lrsw4[]={\")\n",
    "for i in range(np_lrsw4.shape[0]):\n",
    "    f.write(str(int(np_lrsw4[i])))\n",
    "    if(i==np_lrsw4.shape[0]-1):\n",
    "        pass\n",
    "    else:\n",
    "        f.write(',')\n",
    "f.write('};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    img,label=test_data[i]\n",
    "    img=img.reshape(-1)\n",
    "    img=int_conver(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
